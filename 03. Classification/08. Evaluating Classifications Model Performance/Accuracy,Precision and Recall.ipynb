{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True vs False and Postive vs Negative\n",
    "**A short Fable**<br>\n",
    "```\n",
    "An Aesop's Fable: The Boy Who Cried Wolf (compressed)<br>\n",
    "A shepherd boy gets bored tending the town's flock. To have some fun, he cries out, \"Wolf!\" even though no wolf is in sight. The villagers run to protect the flock, but then get really mad when they realize the boy was playing a joke on them.<br><br>\n",
    "One night, the shepherd boy sees a real wolf approaching the flock and calls out, \"Wolf!\" The villagers refuse to be fooled again and stay in their houses. The hungry wolf turns the flock into lamb chops. The town goes hungry. Panic ensues.\n",
    "```\n",
    "Let's make the following definitions:<br>\n",
    "* \"Wolf\" is a positive class.\n",
    "* \"No wolf\" is a negative class.\n",
    "<br><br>\n",
    "We can summarize our \"wolf-prediction\" model using a 2x2 confusion matrix that depicts all four possible outcomes:<br>\n",
    "<img src=\"image.jpg\" width=\"550\" height=\"250\"><br>\n",
    "A **true positive** is an outcome where the model **correctly** predicts the **positive class**. Similarly, a **true negative** is an outcome where the model **correctly** predicts the **negative class**.<br><br>\n",
    "A **false positive** is an outcome where the model **incorrectly** predicts the **positive class**. And a **false negative** is an outcome where the model **incorrectly** predicts the **negative class**.<br>\n",
    "***\n",
    "# Accuracy\n",
    "Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. Formally, accuracy has the following definition:<br>\n",
    "$Accuracy = \\frac{Number \\ of \\ correct \\ Prediction}{Total \\ Number \\ of \\ Predictions}$\n",
    "<br>\n",
    "For binary classification, accuracy can also be calculated in terms of positives and negatives as follows:<br>\n",
    "$Accuracy = \\frac{TP \\ + \\ TN}{TP \\ + \\ TN \\ + \\ FP \\ + FN}$<br>\n",
    "Where `TP = True Positives`, `TN = True Negatives`, `FP = False Positives`, and `FN = False Negatives`.<br>\n",
    "# Precision\n",
    "Precision attempts to answer the following question:<br>\n",
    "`What proportion of positive identifications was actually correct?`<br>\n",
    "$Precision = \\frac{TP}{TP\\ +\\ FP}$\n",
    "# Recall\n",
    "Recall attempts to answer the following question:<br>\n",
    "`What proportion of actual positives was identified correctly?`<br>\n",
    "$Recall = \\frac{TP}{TP\\ +\\ FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
