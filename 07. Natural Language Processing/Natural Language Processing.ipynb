{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "dataset = pd.read_csv('Dataset\\\\Restaurant_Reviews.tsv', delimiter='\\t', quoting=3)\n",
    "#Our file is a tsv file but we are reading it as csv file so we need to pass a delimiter as \"\\t\"\n",
    "#In future we don't need a trouble due to \" \"(quotes) so will ignore using command 'qoting'\n",
    "# 3 is for ignoring quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use .tsv(tab separated delimiter) file because in NLP dataset one can use ,(comma) as punctuation but not tab. Using comma could delimit the sentence where it is not required.<br>\n",
    "After importing the dataset we need to clean the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset there are two columns. First column consists of reviews and second column consits whether the review was positive or negative.<br>\n",
    "Our target is to make a machine learning model which will make a prediction wether the new review is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning the Texts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need :**<br>\n",
    "Our target is to create a bag of words model, which will only consists of only the relevant words and different reviews.<br>\n",
    "* We will get rid of all the non-useful words like *that*,*or*,*on* etc.\n",
    "* we will als get rid of punctuation marks.\n",
    "* We also get rid of numbers unless they are significant impact.<br>\n",
    "We will also do a process known as `stemming` which consists of taking the route of some different versions of a some word. We perform stemming so that we can group similar meaning words.\n",
    "* We will also get rid of text in the Upper case.<br>\n",
    "The last process which will make bag of words model is the tokenization process. <br>\n",
    "`Tokenization` :- It splits  all the different reviews into different words.<br><br>\n",
    "We will take all the word of the different reviews and we will attribute each column for each word. So we will have a lot of column and then for each review each review will contain the number of times the associated word appears in the review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps of cleaning:<br>\n",
    "1. Only keeping the letters in the review. \n",
    " * We will remove the numbers, punctuations etc. except for the alphabets. \n",
    "2. Putting all the letters to lower case.\n",
    "3. Remove non- significant words, i.e. the words which are not relevant in predicting. Words like the, that, and, in, all etc. as we know that a sparse matrix will be created with each column having a word.\n",
    "4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wow... Loved this place.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retaing only alphabets** and **Removing irrelevant words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopword list<br>\n",
    "The library will contain a list of words which are generically irrelevant words. We will see words from the list and delete the irrelevant words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the word is present in the stopword list we will remove it from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Samir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "review = re.sub('[^a-zA-Z]',' ',dataset['Review'][0],)\n",
    "#Here we are working only for first review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with only alphabets left, numbers and punctuation being removed :  Wow    Loved this place \n"
     ]
    }
   ],
   "source": [
    "print(\"Text with only alphabets left, numbers and punctuation being removed : \",review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :<br>\n",
    "* `^` stands for not.\n",
    "* The first parameter is what we want to remove in the text.\n",
    "* we will put what we don't want to remove\n",
    "* 2nd parameter is where we want to remove the text. \n",
    "* There may be a case that if we remove a letter from middle the letter left and right to it may stick and might be undesirable.\n",
    "    * example first second third can result to firstthird on removal of second\n",
    "* so we provide a space by providing empty  `' '`, so the removed character will be replaced by the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing case into lower case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = review.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case changed to lower case of the above text :  wow    loved this place \n"
     ]
    }
   ],
   "source": [
    "print('Case changed to lower case of the above text : ',review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the words from string and put them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "review =review.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string being converted into list  ['wow', 'loved', 'this', 'place']\n"
     ]
    }
   ],
   "source": [
    "print('string being converted into list ',review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "review = [word for word in review if not word in set(stopwords.words('english'))]# List comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will take a maatch with nltk stopwords library and remove the word which are not significant.<br>\n",
    "Use of `set function` here : It will take `stopwords.words('english')` as an `argument` or `input`.<br>\n",
    "We are doing this because it will be way faster for our algorithm to go through all the different word in the package. Because it will traverse faster in `set` than in the `list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of relevant words :  ['wow', 'loved', 'place']\n"
     ]
    }
   ],
   "source": [
    "print('Set of relevant words : ',review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 'this' was irrelevant so it was removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**<br>\n",
    "Stemming is about taking the root of the words.<br>\n",
    "example the word loved comes from the word love, as the word love, loved or loves give the same meaning.<br>\n",
    "We do this to reduce the words in sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]#This step can be done \n",
    "#in the above line while we were implementing stopwords on different wordsby making a small change in the code above.\n",
    "#(repeated for learning process only.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After aplying stemming on the above colection of words  ['wow', 'love', 'place']\n"
     ]
    }
   ],
   "source": [
    "print('After aplying stemming on the above colection of words ',review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting list to string back again\n",
    "review = ' '.join(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List converted back to string :  wow love place\n"
     ]
    }
   ],
   "source": [
    "print('List converted back to string : ',review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above step has been done for only one line but we need to apply this method to whole dataset.(We don't need to do them separately, only for learning process we have done it in two steps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in range(0,1000):\n",
    "    review = re.sub('[^a-zA-Z]',' ',dataset['Review'][i],)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)#We need to append clean review to corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow love place',\n",
       " 'crust good',\n",
       " 'tasti textur nasti',\n",
       " 'stop late may bank holiday rick steve recommend love',\n",
       " 'select menu great price']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]#Getting first 5 results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the **Bag of Word Model**<br>\n",
    "We will create bag of word model of unique letter and fit each letter in each column. Where the number of rows will be equal to 1000. And the cell will be having a **number** which is the count of occurrence of that particular word. <br>\n",
    "So we will basically get a table with a lot of zeroes because there will be many word that have only occured once.<br>\n",
    "A matrix containg a lot of zeroes is called a sparse matrix. And the fact that we have lots of zeroes is called sparsity.<br>\n",
    "**Bag of word model** is creation of **sparse matrix** through the process of **tokenization**. <br>\n",
    "We are going to do classification as the is the review is either is either 0 or 1. Note : The number of words in the corpus will be equal to the total number of independent variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for starting of tokenization\n",
    "cv = CountVectorizer(max_features=1500)# Number of relevent words that we want to keep\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:,1].values#Taking the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]#displaying top 10 dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1500)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that the name of the people are irrelevant words in the model. `max_feature` parameter will allow us to filter the irrelevent words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55, 42],\n",
       "       [12, 91]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model made 55 correct prediction of negative reviews, 91 correct prediction of postive reviews 42 incorrect predictions of positive reviews and 12 incorrent prediction of negative reviews out of total 200 reviews.\n"
     ]
    }
   ],
   "source": [
    "total = cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1]\n",
    "print('Our model made',cm[0,0],'correct prediction of negative reviews,',cm[1,1],'correct prediction of postive reviews',\n",
    "     cm[0,1],'incorrect predictions of positive reviews and',cm[1,0],'incorrent prediction of negative reviews out of total',\n",
    "     total,'reviews.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So number of correct prediction is  146 \n",
      "and number of incorrect predictions is  54\n"
     ]
    }
   ],
   "source": [
    "print(\"So number of correct prediction is \",cm[0,0]+cm[1,1],'\\nand number of incorrect predictions is ',cm[0,1]+cm[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is  0.73 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of the model is ',(cm[0,0]+cm[1,1])/total,'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 570.85,
   "position": {
    "height": "40px",
    "left": "695px",
    "right": "20px",
    "top": "114px",
    "width": "567px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
