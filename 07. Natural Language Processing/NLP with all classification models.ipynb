{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "dataset = pd.read_csv('Dataset\\\\Restaurant_Reviews.tsv', delimiter='\\t', quoting=3)\n",
    "#Our file is a tsv file but we are reading it as csv file so we need to pass a delimiter as \"\\t\"\n",
    "#In future we don't need a trouble due to \" \"(quotes) so will ignore using command 'qoting'\n",
    "# 3 is for ignoring quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Samir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "review = re.sub('[^a-zA-Z]',' ',dataset['Review'][0],)\n",
    "#Here we are working only for first review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text with only alphabets left, numbers and punctuation being removed :  Wow    Loved this place \n"
     ]
    }
   ],
   "source": [
    "print(\"Text with only alphabets left, numbers and punctuation being removed : \",review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = review.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case changed to lower case of the above text :  wow    loved this place \n"
     ]
    }
   ],
   "source": [
    "print('Case changed to lower case of the above text : ',review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "review =review.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string being converted into list  ['wow', 'loved', 'this', 'place']\n"
     ]
    }
   ],
   "source": [
    "print('string being converted into list ',review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "review = [word for word in review if not word in set(stopwords.words('english'))]# List comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]#This step can be done \n",
    "#in the above line while we were implementing stopwords on different wordsby making a small change in the code above.\n",
    "#(repeated for learning process only.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After aplying stemming on the above colection of words  ['wow', 'love', 'place']\n"
     ]
    }
   ],
   "source": [
    "print('After aplying stemming on the above colection of words ',review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting list to string back again\n",
    "review = ' '.join(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List converted back to string :  wow love place\n"
     ]
    }
   ],
   "source": [
    "print('List converted back to string : ',review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in range(0,1000):\n",
    "    review = re.sub('[^a-zA-Z]',' ',dataset['Review'][i],)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)#We need to append clean review to corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow love place',\n",
       " 'crust good',\n",
       " 'tasti textur nasti',\n",
       " 'stop late may bank holiday rick steve recommend love',\n",
       " 'select menu great price']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]#Getting first 5 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for starting of tokenization\n",
    "cv = CountVectorizer(max_features=1500)# Number of relevent words that we want to keep\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:,1].values#Taking the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]#displaying top 10 dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1500)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying different clasification Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55, 42],\n",
       "       [12, 91]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1]\n",
    "correct = cm[0,0]+cm[1,1]\n",
    "incorrect = cm[0,1]+cm[1,0]\n",
    "accuracy = correct/total\n",
    "precision = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "recall = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "f1_score = 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model made 55 correct prediction of negative reviews, 91 correct prediction of postive reviews 42 incorrect predictions of positive reviews and 12 incorrect prediction of negative reviews out of total 200 reviews.\n",
      "\n",
      "So number of correct prediction is  146 \n",
      "and number of incorrect predictions is  54\n",
      "\n",
      "Accuracy =  0.73\n",
      "\n",
      "Precision =  0.5670103092783505\n",
      "\n",
      "Recall =  0.8208955223880597\n",
      "\n",
      "F1 Score =  0.6707317073170731\n"
     ]
    }
   ],
   "source": [
    "print('Our model made',cm[0,0],'correct prediction of negative reviews,',cm[1,1],'correct prediction of postive reviews',\n",
    "     cm[0,1],'incorrect predictions of positive reviews and',cm[1,0],'incorrect prediction of negative reviews out of total',\n",
    "     total,'reviews.\\n')\n",
    "print(\"So number of correct prediction is \",correct,'\\nand number of incorrect predictions is ',incorrect)\n",
    "\n",
    "print('\\nAccuracy = ',accuracy)\n",
    "print('\\nPrecision = ',precision)\n",
    "print('\\nRecall = ',recall)\n",
    "print('\\nF1 Score = ',f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samir\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[76, 21],\n",
       "       [37, 66]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model made 76 correct prediction of negative reviews, 66 correct prediction of postive reviews 21 incorrect predictions of positive reviews and 37 incorrect prediction of negative reviews out of total 200 reviews.\n",
      "\n",
      "So number of correct prediction is  142 \n",
      "and number of incorrect predictions is  58\n",
      "\n",
      "Accuracy =  0.71\n",
      "\n",
      "Precision =  0.7835051546391752\n",
      "\n",
      "Recall =  0.672566371681416\n",
      "\n",
      "F1 Score =  0.7238095238095238\n"
     ]
    }
   ],
   "source": [
    "total = cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1]\n",
    "correct = cm[0,0]+cm[1,1]\n",
    "incorrect = cm[0,1]+cm[1,0]\n",
    "accuracy = correct/total\n",
    "precision = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "recall = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "print('Our model made',cm[0,0],'correct prediction of negative reviews,',cm[1,1],'correct prediction of postive reviews',\n",
    "     cm[0,1],'incorrect predictions of positive reviews and',cm[1,0],'incorrect prediction of negative reviews out of total',\n",
    "     total,'reviews.\\n')\n",
    "print(\"So number of correct prediction is \",correct,'\\nand number of incorrect predictions is ',incorrect)\n",
    "\n",
    "print('\\nAccuracy = ',accuracy)\n",
    "print('\\nPrecision = ',precision)\n",
    "print('\\nRecall = ',recall)\n",
    "print('\\nF1 Score = ',f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[74, 23],\n",
       "       [55, 48]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model made 74 correct prediction of negative reviews, 48 correct prediction of postive reviews 23 incorrect predictions of positive reviews and 55 incorrect prediction of negative reviews out of total 200 reviews.\n",
      "\n",
      "So number of correct prediction is  122 \n",
      "and number of incorrect predictions is  78\n",
      "\n",
      "Accuracy =  0.61\n",
      "\n",
      "Precision =  0.7628865979381443\n",
      "\n",
      "Recall =  0.5736434108527132\n",
      "\n",
      "F1 Score =  0.654867256637168\n"
     ]
    }
   ],
   "source": [
    "total = cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1]\n",
    "correct = cm[0,0]+cm[1,1]\n",
    "incorrect = cm[0,1]+cm[1,0]\n",
    "accuracy = correct/total\n",
    "precision = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "recall = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "print('Our model made',cm[0,0],'correct prediction of negative reviews,',cm[1,1],'correct prediction of postive reviews',\n",
    "     cm[0,1],'incorrect predictions of positive reviews and',cm[1,0],'incorrect prediction of negative reviews out of total',\n",
    "     total,'reviews.\\n')\n",
    "print(\"So number of correct prediction is \",correct,'\\nand number of incorrect predictions is ',incorrect)\n",
    "\n",
    "print('\\nAccuracy = ',accuracy)\n",
    "print('\\nPrecision = ',precision)\n",
    "print('\\nRecall = ',recall)\n",
    "print('\\nF1 Score = ',f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[74, 23],\n",
       "       [33, 70]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model made 74 correct prediction of negative reviews, 70 correct prediction of postive reviews 23 incorrect predictions of positive reviews and 33 incorrect prediction of negative reviews out of total 200 reviews.\n",
      "\n",
      "So number of correct prediction is  144 \n",
      "and number of incorrect predictions is  56\n",
      "\n",
      "Accuracy =  0.72\n",
      "\n",
      "Precision =  0.7628865979381443\n",
      "\n",
      "Recall =  0.6915887850467289\n",
      "\n",
      "F1 Score =  0.7254901960784315\n"
     ]
    }
   ],
   "source": [
    "total = cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1]\n",
    "correct = cm[0,0]+cm[1,1]\n",
    "incorrect = cm[0,1]+cm[1,0]\n",
    "accuracy = correct/total\n",
    "precision = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "recall = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "print('Our model made',cm[0,0],'correct prediction of negative reviews,',cm[1,1],'correct prediction of postive reviews',\n",
    "     cm[0,1],'incorrect predictions of positive reviews and',cm[1,0],'incorrect prediction of negative reviews out of total',\n",
    "     total,'reviews.\\n')\n",
    "print(\"So number of correct prediction is \",correct,'\\nand number of incorrect predictions is ',incorrect)\n",
    "\n",
    "print('\\nAccuracy = ',accuracy)\n",
    "print('\\nPrecision = ',precision)\n",
    "print('\\nRecall = ',recall)\n",
    "print('\\nF1 Score = ',f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernal SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samir\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 97,   0],\n",
       "       [103,   0]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting classifier to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf',random_state = 0)\n",
    "classifier.fit(X_train,y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model made 97 correct prediction of negative reviews, 0 correct prediction of postive reviews 0 incorrect predictions of positive reviews and 103 incorrect prediction of negative reviews out of total 200 reviews.\n",
      "\n",
      "So number of correct prediction is  97 \n",
      "and number of incorrect predictions is  103\n",
      "\n",
      "Accuracy =  0.485\n",
      "\n",
      "Precision =  1.0\n",
      "\n",
      "Recall =  0.485\n",
      "\n",
      "F1 Score =  0.6531986531986532\n"
     ]
    }
   ],
   "source": [
    "total = cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1]\n",
    "correct = cm[0,0]+cm[1,1]\n",
    "incorrect = cm[0,1]+cm[1,0]\n",
    "accuracy = correct/total\n",
    "precision = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "recall = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "print('Our model made',cm[0,0],'correct prediction of negative reviews,',cm[1,1],'correct prediction of postive reviews',\n",
    "     cm[0,1],'incorrect predictions of positive reviews and',cm[1,0],'incorrect prediction of negative reviews out of total',\n",
    "     total,'reviews.\\n')\n",
    "print(\"So number of correct prediction is \",correct,'\\nand number of incorrect predictions is ',incorrect)\n",
    "\n",
    "print('\\nAccuracy = ',accuracy)\n",
    "print('\\nPrecision = ',precision)\n",
    "print('\\nRecall = ',recall)\n",
    "print('\\nF1 Score = ',f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[74, 23],\n",
       "       [35, 68]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting classifier to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion= \"entropy\",random_state=0)\n",
    "classifier.fit(X_train,y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model made 74 correct prediction of negative reviews, 68 correct prediction of postive reviews 23 incorrect predictions of positive reviews and 35 incorrect prediction of negative reviews out of total 200 reviews.\n",
      "\n",
      "So number of correct prediction is  142 \n",
      "and number of incorrect predictions is  58\n",
      "\n",
      "Accuracy =  0.71\n",
      "\n",
      "Precision =  0.7628865979381443\n",
      "\n",
      "Recall =  0.6788990825688074\n",
      "\n",
      "F1 Score =  0.7184466019417477\n"
     ]
    }
   ],
   "source": [
    "total = cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1]\n",
    "correct = cm[0,0]+cm[1,1]\n",
    "incorrect = cm[0,1]+cm[1,0]\n",
    "accuracy = correct/total\n",
    "precision = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "recall = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "print('Our model made',cm[0,0],'correct prediction of negative reviews,',cm[1,1],'correct prediction of postive reviews',\n",
    "     cm[0,1],'incorrect predictions of positive reviews and',cm[1,0],'incorrect prediction of negative reviews out of total',\n",
    "     total,'reviews.\\n')\n",
    "print(\"So number of correct prediction is \",correct,'\\nand number of incorrect predictions is ',incorrect)\n",
    "\n",
    "print('\\nAccuracy = ',accuracy)\n",
    "print('\\nPrecision = ',precision)\n",
    "print('\\nRecall = ',recall)\n",
    "print('\\nF1 Score = ',f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[87, 10],\n",
       "       [46, 57]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting classifier to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=10,criterion=\"entropy\",random_state=0)\n",
    "classifier.fit(X_train,y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model made 87 correct prediction of negative reviews, 57 correct prediction of postive reviews 10 incorrect predictions of positive reviews and 46 incorrect prediction of negative reviews out of total 200 reviews.\n",
      "\n",
      "So number of correct prediction is  144 \n",
      "and number of incorrect predictions is  56\n",
      "\n",
      "Accuracy =  0.72\n",
      "\n",
      "Precision =  0.8969072164948454\n",
      "\n",
      "Recall =  0.6541353383458647\n",
      "\n",
      "F1 Score =  0.7565217391304349\n"
     ]
    }
   ],
   "source": [
    "total = cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1]\n",
    "correct = cm[0,0]+cm[1,1]\n",
    "incorrect = cm[0,1]+cm[1,0]\n",
    "accuracy = correct/total\n",
    "precision = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "recall = cm[0,0]/(cm[0,0]+cm[1,0])\n",
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "print('Our model made',cm[0,0],'correct prediction of negative reviews,',cm[1,1],'correct prediction of postive reviews',\n",
    "     cm[0,1],'incorrect predictions of positive reviews and',cm[1,0],'incorrect prediction of negative reviews out of total',\n",
    "     total,'reviews.\\n')\n",
    "print(\"So number of correct prediction is \",correct,'\\nand number of incorrect predictions is ',incorrect)\n",
    "\n",
    "print('\\nAccuracy = ',accuracy)\n",
    "print('\\nPrecision = ',precision)\n",
    "print('\\nRecall = ',recall)\n",
    "print('\\nF1 Score = ',f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can conclude the various accuracy and f1 scores :<br><br>\n",
    "Naive Bayes : Accuracy = 73% F1-Score = 67%<br>\n",
    "Logistic Regression : Accuracy = 71% F1-Score = 72%<br>\n",
    "K-NN : Accuracy = 61% F1-Score = 65% <br>\n",
    "SVM : Accuracy = 72% F1-Score = 72% <br>\n",
    "kernel-SVM : Accuracy = 48% F1-Score = 65% <br>\n",
    "Decision Tree: Accuracy = 71% F1-Score = 71% <br>\n",
    "Random Forest: Accuracy = 72% F1-Score = 75% <br><br>\n",
    "So as we can see that the best model was the Random Forest Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 570.85,
   "position": {
    "height": "602.85px",
    "left": "728px",
    "right": "20px",
    "top": "123px",
    "width": "511.533px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
